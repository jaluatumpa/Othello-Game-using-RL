{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60aed577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111a0fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6x6 board othello game\n",
    "n=6\n",
    "class Board:\n",
    "    def __init__(self, player1, player2):\n",
    "        self.n = n\n",
    "        self.board = [[0] * n for _ in range(n)]\n",
    "        self.current_play = 1\n",
    "        self.reset()\n",
    "        self.player1 = player1\n",
    "        self.player2 = player2\n",
    "        self.done = False\n",
    "        self.directions = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n",
    "\n",
    "    def reset(self):\n",
    "        n = self.n\n",
    "        self.board = [[0] * n for _ in range(n)]\n",
    "        self.current_play = 1\n",
    "        if n % 2 == 0:\n",
    "            mid = n // 2\n",
    "            self.board[mid - 1][mid - 1] = 1\n",
    "            self.board[mid][mid] = 1\n",
    "            self.board[mid - 1][mid] = -1\n",
    "            self.board[mid][mid - 1] = -1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.board[index]\n",
    "\n",
    "    def countDiff(self, color):\n",
    "        count = 0\n",
    "        for j in range(self.n):\n",
    "            for i in range(self.n):\n",
    "                if self.board[i][j] == color:\n",
    "                    count += 1\n",
    "                if self.board[i][j] == -color:\n",
    "                    count -= 1\n",
    "        return count\n",
    "\n",
    "    def get_legal_moves(self, color):\n",
    "        moves = set()  # stores the legal moves.\n",
    "        for j in range(self.n):\n",
    "            for i in range(self.n):\n",
    "                if self.board[i][j] == color:\n",
    "                    newmoves = self.get_moves_for_square((i, j))\n",
    "                    moves.update(newmoves)\n",
    "        return list(moves)\n",
    "\n",
    "    def has_legal_moves(self, color):#check legal movel\n",
    "        for j in range(self.n):\n",
    "            for i in range(self.n):\n",
    "                if self.board[i][j] == color:\n",
    "                    newmoves = self.get_moves_for_square((i, j))\n",
    "                    if len(newmoves) > 0:\n",
    "                        return True\n",
    "        return False\n",
    "\n",
    "    def get_moves_for_square(self, square):\n",
    "        (i, j) = square\n",
    "        color = self.board[i][j]\n",
    "        if color == 0:\n",
    "            return None\n",
    "        # search all possible directions.\n",
    "        moves = []\n",
    "        for direction in self.directions:\n",
    "            move = self._discover_move(square, direction)\n",
    "            if move:\n",
    "                moves.append(move)\n",
    "        # return the generated move list\n",
    "        return moves\n",
    "\n",
    "    def execute_move(self, move, color):\n",
    "        flips = []\n",
    "        for direction in self.directions:\n",
    "            flip = self._get_flips(move, direction, color)\n",
    "            flips.extend(flip)\n",
    "\n",
    "        if flips:\n",
    "            for i, j in flips:\n",
    "                self.board[i][j] = color\n",
    "        else:\n",
    "            raise Exception(\"Invalid move\")\n",
    "        row, col = move\n",
    "        self.board[row][col] = color\n",
    "        \n",
    "    #valid move in a specified direction starting from the origin position on the game board\n",
    "    def _discover_move(self, origin, direction):\n",
    "        i, j = origin\n",
    "        color = self.board[i][j]\n",
    "        flips = []\n",
    "        for new_i, new_j in Board._increment_move(origin, direction, self.n):\n",
    "            if self.board[new_i][new_j] == 0:\n",
    "                if flips:\n",
    "                    return (new_i, new_j)\n",
    "                else:\n",
    "                    return None\n",
    "            elif self.board[new_i][new_j] == color:\n",
    "                return None\n",
    "            elif self.board[new_i][new_j]== -color:\n",
    "                flips.append((new_i, new_j))\n",
    "        return None\n",
    "\n",
    "    def _get_flips(self, origin, direction, color):\n",
    "        flips = []\n",
    "        for i, j in Board._increment_move(origin, direction, self.n):\n",
    "            if self.board[i][j] == 0:\n",
    "                return []\n",
    "            if self.board[i][j] == -color:\n",
    "                flips.append((i, j))\n",
    "            elif self.board[i][j] == color and len(flips) > 0:\n",
    "                return flips\n",
    "        return []\n",
    "    \n",
    "    #increment current position of the move inthe specified position\n",
    "    def _increment_move(move, direction, n):\n",
    "        i, j = move\n",
    "        move = (i + direction[0], j + direction[1])#adding the next position\n",
    "        while 0 <= move[0] < n and 0 <= move[1] < n:\n",
    "            yield move\n",
    "            move = (move[0] + direction[0], move[1] + direction[1])\n",
    "            \n",
    "    def step(self, action):\n",
    "        i, j = action\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        if i == -1 and j == -1:\n",
    "            self.current_play *= -1\n",
    "            self.done = True\n",
    "            reward = self.get_reward()\n",
    "        elif not self.has_legal_moves(self.current_play):\n",
    "            print(\"Invalid Move!\")\n",
    "            self.done = True\n",
    "            reward = -1 * self.current_play  \n",
    "        else:\n",
    "            self.execute_move((i, j), self.current_play)\n",
    "            self.current_play *= -1\n",
    "\n",
    "        if not self.has_legal_moves(self.current_play):\n",
    "            print(\"No legal moves for the current player\")\n",
    "            self.current_play *= -1\n",
    "\n",
    "        return copy.deepcopy(self.board), reward, self.done, {}\n",
    "\n",
    "\n",
    "    def get_reward(self):\n",
    "        player1_score = self.countDiff(1)\n",
    "        player2_score = self.countDiff(-1)\n",
    "        if player1_score > player2_score:\n",
    "            return 1  # Player 1 (Black) wins\n",
    "        elif player1_score < player2_score:\n",
    "            return -1  # Player 2 (White) wins\n",
    "        else:\n",
    "            return 0  # It's a draw\n",
    "\n",
    "    def game_over(self):\n",
    "        return not self.has_legal_moves(1) and not self.has_legal_moves(-1)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef62e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3222b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent experience\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb27f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DQN agent class\n",
    "class DQNAgent:\n",
    "    #Initialization the hyperparameter\n",
    "    def __init__(self, color, n=6):\n",
    "        self.n = n\n",
    "        self.replay_buffer_size = 200000\n",
    "        self.current_play = 1\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.95\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1\n",
    "        self.min_epsilon = 0.1\n",
    "        self.max_epsilon = 1\n",
    "        self.update_rate = 1000\n",
    "        self.color = color\n",
    "        self.steps = 0\n",
    "        self.lambd = 0.0005\n",
    "        self.train_frequency = 10\n",
    "        self.weight_backup = \"nn_model.h5\" #weight\n",
    "        self.memory = ReplayBuffer(self.replay_buffer_size) #store memory\n",
    "        self.nn_model = self.neural_network() #Q network\n",
    "        self.target_model = self.neural_network() #target network\n",
    "        \n",
    "    ## Qnetwork architecture:The Q network is the agent that is trained to produce the Optimal State-Action value.   \n",
    "    def neural_network(self):\n",
    "        model = Sequential([\n",
    "            Flatten(input_shape=(self.n, self.n)),\n",
    "            Dense(18, activation='relu'),\n",
    "            Dense(9, activation='relu'),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "\n",
    "        if os.path.isfile(self.weight_backup):\n",
    "            model.load_weights(self.weight_backup)\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.nn_model.get_weights())\n",
    "\n",
    "    def get_hash(self, board):\n",
    "        return np.array(board)\n",
    "\n",
    "    def choose_action(self, board):\n",
    "        positions = board.get_legal_moves(self.color)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.choice(positions)\n",
    "        else:\n",
    "            state_old = self.get_hash(board)\n",
    "            pred = self.predict(state_old.reshape((1, self.n, self.n)))[0]\n",
    "#             print(pred)\n",
    "            return positions[np.argmax(pred)]\n",
    "\n",
    "    def remember(self, board, action, reward, next_board, done):\n",
    "        state = self.get_hash(board)\n",
    "        next_state, updated_reward, _, _ = board.step(action)\n",
    "        experience = (state, action, updated_reward, next_state, done)\n",
    "        self.memory.add(experience)\n",
    "        if self.steps % self.train_frequency == 0:\n",
    "            self.replay()\n",
    "            \n",
    "        ##updated target network\n",
    "        if self.steps % self.update_rate == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "        self.steps += 1\n",
    "        self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * math.exp(-self.lambd * self.steps)\n",
    "    \n",
    "    def predict(self, board):\n",
    "        return self.nn_model.predict(board)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory.buffer) < self.batch_size:\n",
    "            return\n",
    "        minibatch = self.memory.sample(self.batch_size) #qnetworrk takes the current state and action\n",
    "        curr_state = np.array([transition[0] for transition in minibatch])\n",
    "        curr_qvalue = self.nn_model.predict(curr_state)\n",
    "        new_curr_state = np.array([transition[3] for transition in minibatch])\n",
    "        future_qvalue = self.target_model.predict(new_curr_state)  #target network takes takes the next state from each data sample and predict the best q value     \n",
    "        x, y = [], []\n",
    "\n",
    "        for idx, (board, action, reward, next_board, done) in enumerate(minibatch):\n",
    "            if not done:\n",
    "                #Bellman equation\n",
    "                target = reward + self.gamma * np.max(future_qvalue[idx])\n",
    "            else:\n",
    "                target = reward\n",
    "\n",
    "            curr_qs = curr_qvalue[idx]\n",
    "            curr_qs[action] = (1 - self.learning_rate) * curr_qs[action] + self.learning_rate * target\n",
    "            x.append(board)\n",
    "            y.append(curr_qs)\n",
    "\n",
    "        self.nn_model.fit(np.array(x), np.array(y), batch_size=self.batch_size, verbose=0, shuffle=True)\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        self.nn_model.save(self.weight_backup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64643661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(player1, player2, num_games):\n",
    "    wins_player1 = 0\n",
    "    wins_player2 = 0\n",
    "    draws = 0\n",
    "    game_durations = []\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        game = Board(player1, player2)\n",
    "        done = False\n",
    "        duration = 0\n",
    "\n",
    "        while not done:\n",
    "            # Player 1's turn\n",
    "            player1_position = game.get_legal_moves(game.current_play)\n",
    "\n",
    "            if not player1_position:\n",
    "                break\n",
    "\n",
    "            if len(player1_position) > 0:\n",
    "                player1_action = player1.choose_action(game)\n",
    "                if isinstance(player1_action, tuple):\n",
    "                    player1_action = player1_action[0]\n",
    "                if 0 <= player1_action < len(player1_position):\n",
    "                    next_state_player1, _, done, _ = game.step(player1_position[player1_action])\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                    duration += 1\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            # Player 2's turn\n",
    "            player2_position = game.get_legal_moves(game.current_play)\n",
    "\n",
    "            if not player2_position:\n",
    "                break\n",
    "\n",
    "            if len(player2_position) > 0:\n",
    "                player2_action = player2.choose_action(game)\n",
    "                if isinstance(player2_action, tuple):\n",
    "                    player2_action = player2_action[0]\n",
    "                if 0 <= player2_action < len(player2_position):\n",
    "                    _, _, done, _ = game.step(player2_position[player2_action])\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        winner = game.get_reward()\n",
    "\n",
    "        if winner == 1:\n",
    "            wins_player1 += 1\n",
    "        elif winner == -1:\n",
    "            wins_player2 += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "        game_durations.append(duration)\n",
    "\n",
    "    win_rate_player1 = wins_player1 / num_games\n",
    "    win_rate_player2 = wins_player2 / num_games\n",
    "    draw_rate = draws / num_games\n",
    "    avg_duration = np.mean(game_durations)\n",
    "\n",
    "    return win_rate_player1, win_rate_player2, draw_rate, avg_duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff128465",
   "metadata": {},
   "outputs": [],
   "source": [
    "player1=DQNAgent(1,6)\n",
    "player2=DQNAgent(-1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2f73093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 Win Rate: 0.6\n",
      "Player 2 Win Rate: 0.1\n",
      "Draw Rate: 0.3\n",
      "Average Game Duration: 5.8\n"
     ]
    }
   ],
   "source": [
    "num_evaluation_games = 10\n",
    "win_rate_player1, win_rate_player2, draw_rate, avg_duration = evaluate(player1, player2, num_evaluation_games)\n",
    "\n",
    "print(f\"Player 1 Win Rate: {win_rate_player1}\")\n",
    "print(f\"Player 2 Win Rate: {win_rate_player2}\")\n",
    "print(f\"Draw Rate: {draw_rate}\")\n",
    "print(f\"Average Game Duration: {avg_duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39074bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f7fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a72097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
