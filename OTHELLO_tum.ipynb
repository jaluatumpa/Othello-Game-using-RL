{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0db026d",
   "metadata": {},
   "source": [
    "Reference:https://github.com/suragnair/alpha-zero-general/tree/master/othello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60aed577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras  \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "tf.keras.utils.disable_interactive_logging()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "986ffc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Game Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "111a0fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Board:\n",
    "    def __init__(self, player1, player2, n=6):\n",
    "        self.n = n\n",
    "        self.board = [[0] * n for _ in range(n)]\n",
    "        self.current_play = 1\n",
    "        self.reset()\n",
    "        self.player1 = player1\n",
    "        self.player2 = player2\n",
    "        self.done = False\n",
    "        self.directions = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n",
    "\n",
    "    def reset(self):\n",
    "        n = self.n\n",
    "        self.board = [[0] * n for _ in range(n)]\n",
    "        self.current_play = 1\n",
    "        if n % 2 == 0:\n",
    "            mid = n // 2\n",
    "            self.board[mid - 1][mid - 1] = 1\n",
    "            self.board[mid][mid] = 1\n",
    "            self.board[mid - 1][mid] = -1\n",
    "            self.board[mid][mid - 1] = -1\n",
    "            \n",
    "    def get_board_state(self):\n",
    "        return self.board \n",
    "\n",
    "#     def getHash(self):\n",
    "#         self.boardHash = str(np.array(self.board).reshape(self.n * self.n))\n",
    "#         return self.boardHash\n",
    "\n",
    "\n",
    "    def count_diff(self, color):\n",
    "        count = 0\n",
    "        for j in range(self.n):\n",
    "            for i in range(self.n):\n",
    "                if self.board[i][j] == color:\n",
    "                    count += 1\n",
    "                if self.board[i][j] == -color:\n",
    "                    count -= 1\n",
    "        return count\n",
    "\n",
    "    def get_legal_moves(self, color):\n",
    "        moves = set()\n",
    "        for j in range(self.n):\n",
    "            for i in range(self.n):\n",
    "                if self.board[i][j] == color:\n",
    "                    new_moves = self.get_moves_for_square((i, j))\n",
    "                    moves.update(new_moves)\n",
    "        return list(moves)\n",
    "\n",
    "    def has_legal_moves(self, color):\n",
    "        for j in range(self.n):\n",
    "            for i in range(self.n):\n",
    "                if self.board[i][j] == color:\n",
    "                    new_moves = self.get_moves_for_square((i, j))\n",
    "                    if len(new_moves) > 0:\n",
    "                        return True\n",
    "        return False\n",
    "\n",
    "    def get_moves_for_square(self, square):\n",
    "        (i, j) = square\n",
    "        color = self.board[i][j]\n",
    "        if color == 0:\n",
    "            return None\n",
    "        moves = []\n",
    "        for direction in self.directions:\n",
    "            move = self._discover_move(square, direction)\n",
    "            if move:\n",
    "                moves.append(move)\n",
    "        return moves\n",
    "\n",
    "    def execute_move(self, move, color):\n",
    "        flips = []\n",
    "        for direction in self.directions:\n",
    "            flip = self._get_flips(move, direction, color)\n",
    "            flips.extend(flip)\n",
    "\n",
    "        if flips:\n",
    "            for i, j in flips:\n",
    "                self.board[i][j] = color\n",
    "        else:\n",
    "            raise Exception(\"Invalid move\")\n",
    "        row, col = move\n",
    "        self.board[row][col] = color\n",
    "\n",
    "    def _discover_move(self, origin, direction):\n",
    "        i, j = origin\n",
    "        color = self.board[i][j]\n",
    "        flips = []\n",
    "        for new_i, new_j in self._increment_move(origin, direction, self.n):\n",
    "            if self.board[new_i][new_j] == 0:\n",
    "                if flips:\n",
    "                    return (new_i, new_j)\n",
    "                else:\n",
    "                    return None\n",
    "            elif self.board[new_i][new_j] == color:\n",
    "                return None\n",
    "            elif self.board[new_i][new_j] == -color:\n",
    "                flips.append((new_i, new_j))\n",
    "        return None\n",
    "\n",
    "    def _get_flips(self, origin, direction, color):\n",
    "        flips = []\n",
    "        for i, j in self._increment_move(origin, direction, self.n):\n",
    "            if self.board[i][j] == 0:\n",
    "                return []\n",
    "            if self.board[i][j] == -color:\n",
    "                flips.append((i, j))\n",
    "            elif self.board[i][j] == color and len(flips) > 0:\n",
    "                return flips\n",
    "        return []\n",
    "\n",
    "    def _increment_move(self, move, direction, n):\n",
    "        i, j = move\n",
    "        move = (i + direction[0], j + direction[1])\n",
    "        while 0 <= move[0] < n and 0 <= move[1] < n:\n",
    "            yield move\n",
    "            move = (move[0] + direction[0], move[1] + direction[1])\n",
    "\n",
    "    def get_reward(self):\n",
    "        player1_score = self.count_diff(1)\n",
    "        player2_score = self.count_diff(-1)\n",
    "        if player1_score > player2_score:\n",
    "            return 1  # Player 1 (Black) wins\n",
    "        elif player1_score < player2_score:\n",
    "            return -1  # Player 2 (White) wins\n",
    "        else:\n",
    "            return 0  # It's a draw\n",
    "\n",
    "    def step(self, action):\n",
    "        i, j = action\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        if i == -1 and j == -1:\n",
    "            self.current_play *= -1\n",
    "            self.done = True\n",
    "            reward = self.get_reward()\n",
    "        elif not self.has_legal_moves(self.current_play):\n",
    "            print(\"Invalid Move!\")\n",
    "            self.done = True\n",
    "            reward = -1 * self.current_play\n",
    "        else:\n",
    "            next_state = copy.deepcopy(self.board)\n",
    "            try:\n",
    "                self.execute_move((i, j), self.current_play)\n",
    "                self.current_play *= -1\n",
    "                done = self.game_over()\n",
    "            except Exception as e:\n",
    "                print(f\"Error in executing move: {e}\")\n",
    "                done = True\n",
    "                reward = -1 * self.current_play\n",
    "\n",
    "        if not done and not self.has_legal_moves(self.current_play):\n",
    "            print(\"No legal moves for the current player.\")\n",
    "            self.current_play *= -1\n",
    "\n",
    "        return copy.deepcopy(self.board) if next_state is None else copy.deepcopy(next_state), reward, done, {}\n",
    "\n",
    "    def game_over(self):\n",
    "        return not self.has_legal_moves(1) and not self.has_legal_moves(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8292af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN agent class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            batch_size = len(self.buffer)\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        self.buffer[idx] = (self.buffer[idx][0], self.buffer[idx][1], self.buffer[idx][2], self.buffer[idx][3], error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e0e996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, color, n=6):\n",
    "        self.n = n\n",
    "        self.current_play = 1\n",
    "        self.color = color\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.95\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1\n",
    "        self.min_epsilon = 0.1\n",
    "        self.max_epsilon = 1\n",
    "        self.update_rate = 5000\n",
    "        self.steps = 0\n",
    "        self.lambd = 0.0005\n",
    "        self.train_frequency = 100\n",
    "        self.weight_backup = \"nn_model.h5\"\n",
    "        self.replay_buffer_size = 20000\n",
    "        self.memory = ReplayBuffer(self.replay_buffer_size)\n",
    "        self.nn_model = self.neural_network()\n",
    "        self.target_model = self.neural_network()\n",
    "\n",
    "    def neural_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(36, input_shape=(self.n*self.n,), activation='relu'))\n",
    "        model.add(Dense(36, activation='relu'))\n",
    "        model.add(Dense(self.n*self.n, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "\n",
    "    def load_model(self):\n",
    "        if os.path.isfile(self.weight_backup):\n",
    "            self.nn_model.load_weights(self.weight_backup)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.nn_model.get_weights())\n",
    "\n",
    "    def get_hash(self, board):\n",
    "        board_hash = np.array(board).flatten()\n",
    "        return board_hash\n",
    "\n",
    "        \n",
    "    def choose_action(self, board):\n",
    "        positions = board.get_legal_moves(self.color)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.choice(positions)\n",
    "        else:\n",
    "            state_old = self.get_hash(board.get_board_state())\n",
    "            pred = self.nn_model.predict(np.array([state_old]))[0]\n",
    "            best_action_index = np.argmax(pred)\n",
    "            best_action = positions[best_action_index]\n",
    "            return best_action\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        experience = (self.get_hash(state), action, reward, self.get_hash(next_state), done)\n",
    "        #print(\"Experience added:\", experience)\n",
    "\n",
    "        self.memory.add(experience)\n",
    "\n",
    "        if self.steps % self.train_frequency == 0:\n",
    "            self.replay()\n",
    "\n",
    "        if self.steps % self.update_rate == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "        self.steps += 1\n",
    "        self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * math.exp(-self.lambd * self.steps)\n",
    "\n",
    "    def predict(self, board, target=False):\n",
    "        state_hash = self.get_hash(board)\n",
    "       \n",
    "        state_hash = np.array([state_hash])  \n",
    "        if target:\n",
    "            return self.target_model.predict(state_hash)\n",
    "        else:\n",
    "            return self.nn_model.predict(state_hash)[0]\n",
    "\n",
    "    def replay_value(self, minibatch):\n",
    "        x = []\n",
    "        y = []\n",
    "        errors = []\n",
    "        for (state, action, reward, next_state, done) in minibatch:\n",
    "            state_hash = self.get_hash(state)\n",
    "            pred = self.predict(state, target=False)[0]\n",
    "\n",
    "            if isinstance(action, tuple):\n",
    "                action_index = action[0] * self.n + action[1]\n",
    "            else:\n",
    "                action_index = action\n",
    "            try:\n",
    "                true_val = pred[action_index]\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "            if not done:\n",
    "                next_pred = self.predict(next_state, target=False)[0]\n",
    "\n",
    "                next_curr_state = self.get_hash(next_state)\n",
    "                next_pred_target = self.predict(next_curr_state, target=True)[0]\n",
    "\n",
    "                pred[action_index] = reward + self.gamma * next_pred_target[np.argmax(next_pred)]\n",
    "            else:\n",
    "                pred[action_index] = reward\n",
    "\n",
    "            error = abs(true_val - pred[action_index])\n",
    "            x.append(state)\n",
    "            y.append(pred)\n",
    "            errors.append(error)\n",
    "\n",
    "        return x, y, errors\n",
    "\n",
    "    def replay(self):\n",
    "        minibatch = self.memory.sample(self.batch_size)\n",
    "        #print(\"Replay buffer size:\", len(self.memory.buffer))\n",
    "        x, y, errors = self.replay_value(minibatch)\n",
    "\n",
    "        for i in range(len(minibatch)):\n",
    "            idx = minibatch[i][0]\n",
    "            if errors:  \n",
    "                self.memory.update(idx, errors[i])\n",
    "\n",
    "        if x and y:  \n",
    "            self.nn_model.fit(np.array(x), np.array(y), batch_size=self.batch_size, epochs=1, verbose=2, shuffle=True)\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        self.nn_model.save(self.weight_backup)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4295422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(player1, player2, num_games):\n",
    "    wins_player1 = 0\n",
    "    wins_player2 = 0\n",
    "    draws = 0\n",
    "    game_durations = []\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        \n",
    "        board = Board(player1.color, player2.color)\n",
    "        done = False\n",
    "        duration = 0  \n",
    "        while not done:\n",
    "            # Player 1's turn\n",
    "            player1_position = board.get_legal_moves(board.current_play)\n",
    "            if not player1_position:\n",
    "                break\n",
    "\n",
    "            if len(player1_position) > 0:\n",
    "                player1_action = player1.choose_action(board)\n",
    "                if isinstance(player1_action, tuple):\n",
    "                    player1_action = player1_action[0]\n",
    "                if 0 <= player1_action < len(player1_position):\n",
    "                    _, _, done, _ = board.step(player1_position[player1_action])\n",
    "\n",
    "                    # Remember the state, action, reward, next_state, and done for player 1\n",
    "                    player1.remember(copy.deepcopy(board.board), player1_action, _, copy.deepcopy(board.board), done)\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            # Player 2's turn\n",
    "            player2_position = board.get_legal_moves(board.current_play)\n",
    "            if not player2_position:\n",
    "                break\n",
    "\n",
    "            if len(player2_position) > 0:\n",
    "                player2_action = player2.choose_action(board)\n",
    "                if isinstance(player2_action, tuple):\n",
    "                    player2_action = player2_action[0]\n",
    "                if 0 <= player2_action < len(player2_position):\n",
    "                    _, _, done, _ = board.step(player2_position[player2_action])\n",
    "\n",
    "                    # Remember the state, action, reward, next_state, and done for player 2\n",
    "                    player2.remember(copy.deepcopy(board.board), player2_action, _, copy.deepcopy(board.board), done)\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            duration += 1\n",
    "\n",
    "        player1.replay()\n",
    "        player2.replay()\n",
    "\n",
    "        winner = board.get_reward()\n",
    "\n",
    "        if winner == 1:\n",
    "            wins_player1 += 1\n",
    "        elif winner == -1:\n",
    "            wins_player2 += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "        game_durations.append(duration)\n",
    "\n",
    "    win_rate_player1 = wins_player1 / num_games\n",
    "    win_rate_player2 = wins_player2 / num_games\n",
    "    draw_rate = draws / num_games\n",
    "    avg_duration = np.mean(game_durations)\n",
    "\n",
    "    return win_rate_player1, win_rate_player2, draw_rate, avg_duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12219f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 Win Rate: 0.6\n",
      "Player 2 Win Rate: 0.2\n",
      "Draw Rate: 0.2\n",
      "Average Game Duration: 4.2\n"
     ]
    }
   ],
   "source": [
    "player1 = DQNAgent(1, 6)\n",
    "player2 = DQNAgent(-1, 6)\n",
    "num_evaluation_games = 10\n",
    "\n",
    "win_rate_player1, win_rate_player2, draw_rate, avg_duration = evaluate(player1, player2, num_evaluation_games)\n",
    "\n",
    "print(f\"Player 1 Win Rate: {win_rate_player1}\")\n",
    "print(f\"Player 2 Win Rate: {win_rate_player2}\")\n",
    "print(f\"Draw Rate: {draw_rate}\")\n",
    "print(f\"Average Game Duration: {avg_duration}\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ef62e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 Win Rate: 0.8\n",
      "Player 2 Win Rate: 0.1\n",
      "Draw Rate: 0.1\n",
      "Average Game Duration: 1.3\n"
     ]
    }
   ],
   "source": [
    "player1 = DQNAgent(1, 6)\n",
    "player2 = DQNAgent(-1, 6)\n",
    "num_evaluation_games = 10\n",
    "\n",
    "win_rate_player1, win_rate_player2, draw_rate, avg_duration = evaluate(player1, player2, num_evaluation_games)\n",
    "\n",
    "print(f\"Player 1 Win Rate: {win_rate_player1}\")\n",
    "print(f\"Player 2 Win Rate: {win_rate_player2}\")\n",
    "print(f\"Draw Rate: {draw_rate}\")\n",
    "print(f\"Average Game Duration: {avg_duration}\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab48ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL agent first and random player as second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d297fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, color, name):\n",
    "        self.color = color\n",
    "        self.name = name\n",
    "\n",
    "    def choose_action(self, board):\n",
    "        legal_moves = board.get_legal_moves(self.color)\n",
    "        if legal_moves:\n",
    "            return random.choice(legal_moves)\n",
    "        return None\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        pass\n",
    "    def replay_value(self, minibatch):\n",
    "        pass\n",
    "    def replay(self):\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e854270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 Win Rate: 0.5\n",
      "Player 2 Win Rate: 0.1\n",
      "Draw Rate: 0.4\n",
      "Average Game Duration: 3.6\n"
     ]
    }
   ],
   "source": [
    "player1 = DQNAgent(1, 6)\n",
    "player2 = Player(-1,\"randomPlayer\")\n",
    "num_evaluation_games = 10\n",
    "\n",
    "win_rate_player1, win_rate_player2, draw_rate, avg_duration = evaluate(player1, player2, num_evaluation_games)\n",
    "\n",
    "print(f\"Player 1 Win Rate: {win_rate_player1}\")\n",
    "print(f\"Player 2 Win Rate: {win_rate_player2}\")\n",
    "print(f\"Draw Rate: {draw_rate}\")\n",
    "print(f\"Average Game Duration: {avg_duration}\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b5860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
